{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GOT_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJIfDwOX11PD",
        "outputId": "6394177e-eb43-4382-98df-296edec752ff"
      },
      "source": [
        "!pip install torch"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5a7fYUF13NE"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDeKKA5U5o9D"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyyTqh7Sv_Lu",
        "outputId": "cb1db140-3767-4e9d-ed9e-48b6f730bf57"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe30QVca5sOx"
      },
      "source": [
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    text = text.split()\n",
        "\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
        "\n",
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]\n",
        "\n",
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True)\n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
        "                torch.zeros(1, batch_size, self.lstm_size))\n",
        "\n",
        "\n",
        "def get_loss_and_train_op(net, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    return criterion, optimizer\n",
        "\n",
        "\n",
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
        "    net.eval()\n",
        "    # Sentence/text to predict. Starts with ____\n",
        "    # Example: \n",
        "    words = [\"I\", \"am\"]\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    print(' '.join(words).encode('utf-8'))\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdHV0jFc_jfu"
      },
      "source": [
        "flags = Namespace(\n",
        "    train_file='/content/drive/MyDrive/resources/jon_snow2.txt',\n",
        "    seq_size=32,\n",
        "    batch_size=16,\n",
        "    embedding_size=64,\n",
        "    lstm_size=64,\n",
        "    gradients_norm=5,\n",
        "    initial_words=[\"I\", \"am\"],\n",
        "    predict_top_k=5,\n",
        "    checkpoint_path='checkpoint',\n",
        ")\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIP4iBi8AIgx",
        "outputId": "e40e3ee2-16d3-4e8d-dd63-e542ead29235"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
        "        flags.train_file, flags.batch_size, flags.seq_size)\n",
        "\n",
        "net = RNNModule(n_vocab, flags.seq_size,\n",
        "                    flags.embedding_size, flags.lstm_size)\n",
        "net = net.to(device)\n",
        "\n",
        "criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
        "\n",
        "iteration = 0\n",
        "\n",
        "for e in range(200):\n",
        "  batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
        "  state_h, state_c = net.zero_state(flags.batch_size)\n",
        "  state_h = state_h.to(device)\n",
        "  state_c = state_c.to(device)\n",
        "  for x, y in batches:\n",
        "    iteration += 1\n",
        "    net.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    x = torch.tensor(x).to(device)\n",
        "    y = torch.tensor(y).to(device)\n",
        "    logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "\n",
        "    loss = criterion(logits.transpose(1, 2), y)\n",
        "    loss_value = loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    state_h = state_h.detach()\n",
        "    state_c = state_c.detach()\n",
        "\n",
        "    _ = torch.nn.utils.clip_grad_norm_(net.parameters(), flags.gradients_norm)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if iteration % 100 == 0:\n",
        "      print('Epoch: {}/{}'.format(e, 200),'Iteration: {}'.format(iteration), 'Loss: {}'.format(loss_value))\n",
        "\n",
        "    if iteration % 1000 == 0:\n",
        "      predict(device, net, flags.initial_words, n_vocab, vocab_to_int, int_to_vocab, top_k=5)\n",
        "      torch.save(net.state_dict(),'model-{}.pth'.format(iteration))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 2820\n",
            "Epoch: 4/200 Iteration: 100 Loss: 4.840061187744141\n",
            "Epoch: 8/200 Iteration: 200 Loss: 3.513777732849121\n",
            "Epoch: 12/200 Iteration: 300 Loss: 2.5344529151916504\n",
            "Epoch: 16/200 Iteration: 400 Loss: 1.8400331735610962\n",
            "Epoch: 20/200 Iteration: 500 Loss: 1.166304588317871\n",
            "Epoch: 24/200 Iteration: 600 Loss: 1.0005416870117188\n",
            "Epoch: 29/200 Iteration: 700 Loss: 0.8616155385971069\n",
            "Epoch: 33/200 Iteration: 800 Loss: 0.6784531474113464\n",
            "Epoch: 37/200 Iteration: 900 Loss: 0.5208595991134644\n",
            "Epoch: 41/200 Iteration: 1000 Loss: 0.37609878182411194\n",
            "b\"I am true enemy won't hit you, no sense of the men who she all I am what Stannis Baratheon would like me outside my life? No. What do whatever always talked into all die. I am Lord Commander Mormont thought we'd lost until you are the Army Rickon first time You ever If you think they'll come? Lady of us too. No. They follow your people. I broke, he just but we're next. of your life for coming to ask him, My Lord, do her again to death. Worse But I'm not a Stark. The winds shot or your cold, rolled Only\"\n",
            "Epoch: 45/200 Iteration: 1100 Loss: 0.16182957589626312\n",
            "Epoch: 49/200 Iteration: 1200 Loss: 0.16948695480823517\n",
            "Epoch: 54/200 Iteration: 1300 Loss: 0.1780301183462143\n",
            "Epoch: 58/200 Iteration: 1400 Loss: 0.12515869736671448\n",
            "Epoch: 62/200 Iteration: 1500 Loss: 0.097508504986763\n",
            "Epoch: 66/200 Iteration: 1600 Loss: 0.07734657824039459\n",
            "Epoch: 70/200 Iteration: 1700 Loss: 0.043429404497146606\n",
            "Epoch: 74/200 Iteration: 1800 Loss: 0.049043916165828705\n",
            "Epoch: 79/200 Iteration: 1900 Loss: 0.05705852061510086\n",
            "Epoch: 83/200 Iteration: 2000 Loss: 0.037366777658462524\n",
            "b\"I am not know. Even there? ever getting King's free to you're each other. We lost until he has is why She you know what they know, right, I have north. to survive. them. The dead north placed me and the only and Cersei. Because and let you do. A girl. We could be a thousand girl. King miles are they'll come? King are walking along my sword. That's her side, counseling Bitterly. betrayed on your family hasn't guards me? I didn't. You have more mammoths. what are not take for any last And why is the Wall to make a man is\"\n",
            "Epoch: 87/200 Iteration: 2100 Loss: 0.03883513808250427\n",
            "Epoch: 91/200 Iteration: 2200 Loss: 0.44492602348327637\n",
            "Epoch: 95/200 Iteration: 2300 Loss: 0.3351280689239502\n",
            "Epoch: 99/200 Iteration: 2400 Loss: 0.11345569044351578\n",
            "Epoch: 104/200 Iteration: 2500 Loss: 0.06435545533895493\n",
            "Epoch: 108/200 Iteration: 2600 Loss: 0.04395343363285065\n",
            "Epoch: 112/200 Iteration: 2700 Loss: 0.03822511062026024\n",
            "Epoch: 116/200 Iteration: 2800 Loss: 0.025754496455192566\n",
            "Epoch: 120/200 Iteration: 2900 Loss: 0.016365880146622658\n",
            "Epoch: 124/200 Iteration: 3000 Loss: 0.019983721897006035\n",
            "b\"I am against of us. We're fire on your own. Mance attacked an army because he holds beyond the North has an oath does then consider the wildlings. Lord Commander thought I need for a southern ruler, asking for a family home now. own. it's been north of the sky. The same to keep the words. I came after me and we're going north at Craster's They're good all I wish You have anything the Wall. Walk. you stop trying at To our She has attacked me, they'll before you. I put an them all. You still here. I'll open us why. come\"\n",
            "Epoch: 129/200 Iteration: 3100 Loss: 0.022864941507577896\n",
            "Epoch: 133/200 Iteration: 3200 Loss: 0.018363190814852715\n",
            "Epoch: 137/200 Iteration: 3300 Loss: 0.0163401048630476\n",
            "Epoch: 141/200 Iteration: 3400 Loss: 0.017946476116776466\n",
            "Epoch: 145/200 Iteration: 3500 Loss: 0.009802083484828472\n",
            "Epoch: 149/200 Iteration: 3600 Loss: 0.010447710752487183\n",
            "Epoch: 154/200 Iteration: 3700 Loss: 0.011619090102612972\n",
            "Epoch: 158/200 Iteration: 3800 Loss: 0.009758222848176956\n",
            "Epoch: 162/200 Iteration: 3900 Loss: 0.009813128970563412\n",
            "Epoch: 166/200 Iteration: 4000 Loss: 0.01716858521103859\n",
            "b\"I am Tyrion Maybe that can Ygritte! came with an few if he said? join it. We have orders. word at the Godswood. of them will stand beside us been as I thought as Ros took you? Jon Snow. It's it back. it DAENERYS Are we need you. I'm ready won't tell me if you're a family I'm a man of the First your shield in then you betrayed him. Sam hit you. You're still word of all in serving as if he was smart, hurt me. I want them monsters. to JON We understand. You brought me be a good queen. That's\"\n",
            "Epoch: 170/200 Iteration: 4100 Loss: 0.7458074688911438\n",
            "Epoch: 174/200 Iteration: 4200 Loss: 0.3325616419315338\n",
            "Epoch: 179/200 Iteration: 4300 Loss: 0.07327418029308319\n",
            "Epoch: 183/200 Iteration: 4400 Loss: 0.028465094044804573\n",
            "Epoch: 187/200 Iteration: 4500 Loss: 0.024292126297950745\n",
            "Epoch: 191/200 Iteration: 4600 Loss: 0.016753027215600014\n",
            "Epoch: 195/200 Iteration: 4700 Loss: 0.010023893788456917\n",
            "Epoch: 199/200 Iteration: 4800 Loss: 0.012630580924451351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsUhQQcnASiC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}